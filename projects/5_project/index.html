<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reshaping Bonsai | Vashisth Tiwari </title> <meta name="author" content="Vashisth Tiwari"> <meta name="description" content="Pruning LLMs for Mathematical Reasoning. Can we prune LLMs while maintaining their mathematical reasoning abilities? How does a novel comprehensive metric affect pruning?"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://vashistht.github.io/projects/5_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand mr-auto" href="/"> <strong>वशिष्ठ </strong>तिवारी </a> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Vashisth</span> Tiwari </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">service </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reshaping Bonsai</h1> <p class="post-description">Pruning LLMs for Mathematical Reasoning. Can we prune LLMs while maintaining their mathematical reasoning abilities? How does a novel comprehensive metric affect pruning?</p> </header> <article> <p>This project, completed for the Advanced Natural Language Processing course, focused on improving the Bonsai <a class="citation" href="#dery2024everybodyprunenowstructured">(Dery et al., 2024)</a> pruning method for Large Language Models (LLMs) with a specific emphasis on mathematical reasoning capabilities. I had the chance to work with two amazing people in Amanda Li and Emily Guo on this project. You can find more about the project <a href="https://github.com/Vashistht/anlp-project" rel="external nofollow noopener" target="_blank">here</a>, where we’ve open-sourced our code and findings.</p> <h3 id="key-concepts">Key Concepts</h3> <ol> <li> <p><strong>Bonsai Pruning</strong>: A forward-only, regression-based neural network pruning method that decides which modules to prune based on estimates of module importance.</p> </li> <li> <p><strong>Comprehensive Metric</strong>: A novel metric combining lexicographical similarity, semantic similarity, and accuracy to evaluate model-generated outputs against ground truth during pruning.</p> </li> </ol> <h2 id="technical-background">Technical Background</h2> <h3 id="bonsai-pruning-method">Bonsai Pruning Method</h3> <p>The Bonsai pruning method aims to solve the following optimization problem:</p> \[m^* = \arg\max_{\bar{m} \in F_p} U(M|_{\bar{m}})\] <p>where \(F_p = \{\bar{m} \subseteq m \mid \sum_{[j:m_j \in \bar{m}]} s_j \leq (1-p)D\}\)</p> <p>Here, \(m^*\) represents the optimal sub-model, \(p\) is the target sparsity, \(U\) is the utility function measuring model performance, and \(D\) is the total number of parameters.</p> <p>Bonsai estimates module importance using a regression-based approach:</p> \[\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^N} \left\{\frac{1}{n}\sum_{(\bar{m}_k, U_k) \in \mathcal{D}} (U_k - \beta^T \alpha_{\bar{m}_k})^2 + \gamma\|\beta\|^2\right\}\] <p>where \(\mathcal{D}\) is the dataset of sampled sub-modules and their performances, and \(\alpha_{\bar{m}_k}\) is a binary mask.</p> <h3 id="our-novel-comprehensive-metric">Our Novel Comprehensive Metric</h3> <p>We introduce a new metric \(U^\dagger\) that combines lexicographical similarity, semantic similarity, and accuracy:</p> \[U^\dagger = \sum_{i=1}^n a_i M_i \quad \text{where} \quad \sum_{i=1}^n a_i = 100\] <p>Here, \(M_i\) represents individual metrics (e.g., lexicographical similarity, semantic similarity, accuracy), and \(a_i\) are their respective weights.</p> <h2 id="research-focus">Research Focus</h2> <p>The project explored various aspects of LLM pruning through several experiments and ablation studies:</p> <ul> <li> <p><strong>Task-Specific Pruning:</strong> Used the GSM-8K dataset, which includes step-by-step reasoning for math problems, to guide the pruning process.</p> </li> <li> <p><strong>Chain-of-Thought Prompting:</strong> Incorporated example questions and “think step by step” instructions in our prompts during pruning. Our prompt structure was:</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Example:{example-question}, Rationale: {example-rationale}, Answer: {example-answer}.
Question: {new-question}
"Let's think step by step to get the rationale and the answer:"
</code></pre></div> </div> </li> <li> <p><strong>Sparsity Levels:</strong> Investigated model performance at different sparsity levels, particularly at 50% and 60% retention.</p> </li> <li> <p><strong>Metric Component Analysis:</strong> Examined the impact of different components in our comprehensive metric:</p> <ul> <li>Lexicographical Similarity (lex_sim): Calculated using F1 score</li> <li>Semantic Similarity (cos_sim): Computed using cosine similarity of sentence embeddings</li> <li>Accuracy (acc): Measured using exact match</li> </ul> </li> </ul> <h2 id="key-findings">Key Findings</h2> <ul> <li> <p>Pruning with GSM-8K led to improved performance on 5 out of 8 downstream reasoning tasks, particularly for logical reasoning datasets.</p> </li> <li> <p>At 60% retention (40% sparsity), our pruned model outperformed the baseline across most datasets, with notable gains in Arc-e, HellaSwag, and MMLU-Elementary Math.</p> </li> <li> <p>For specific weight combinations (38-24-38 for lex_sim, cos_sim, and acc respectively) in our metric, we achieved a 20x lower perplexity on GSM-8K compared to the baseline:</p> <table> <thead> <tr> <th>Model</th> <th>GSM-8K Perplexity</th> </tr> </thead> <tbody> <tr> <td>Baseline</td> <td>365.5226</td> </tr> <tr> <td>Our Model (38-24-38)</td> <td>17.1083</td> </tr> </tbody> </table> </li> <li> <p>Exact match scores (used as a proxy for accuracy) were often close to 0 at high sparsity levels, while semantic similarity scores were consistently high, suggesting potential limitations in our embedding model.</p> </li> </ul> <h2 id="challenges-and-limitations">Challenges and Limitations</h2> <ol> <li> <p><strong>Computational Constraints:</strong> We had to reduce the number of generated tokens from 100 to 20 and increase pruning step size from 5% to 20% per iteration.</p> </li> <li> <p><strong>Accuracy Measurement:</strong> Defining accuracy based on the presence of the ground truth answer string in the output may have led to false positives.</p> </li> <li> <p><strong>Embedding Model Limitations:</strong> The sentence embedding model used for semantic similarity (all-MiniLM-L6-v2) may not have captured the nuances of mathematical reasoning effectively.</p> </li> </ol> <h2 id="future-directions">Future Directions</h2> <ul> <li>Implement more granular pruning steps</li> <li>Explore richer embedding models for semantic similarity, possibly using LLaMA’s own embeddings</li> <li>Conduct thorough exploration of hyperparameters</li> <li>Investigate Bayesian Linear Regression for choosing regression coefficients</li> <li>Explore non-linear combinations of metric components</li> </ul> <p>While our results were mixed, this project demonstrates the potential of using task-specific datasets and comprehensive metrics for pruning language models while maintaining their reasoning capabilities. As we continue to refine our approach, we hope to contribute to the development of more efficient and capable language models for specific reasoning tasks.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="dery2024everybodyprunenowstructured" class="col-sm-8"> <div class="title">Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</div> <div class="author"> Lucio Dery, Steven Kolawole, Jean-François Kagy, Virginia Smith, Graham Neubig, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ameet Talwalkar' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> <style>.abbr .badge{font-size:.8em;white-space:normal;text-align:center;line-height:1.2;padding:.3em .5em;display:inline-block;max-width:100%}</style> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Vashisth Tiwari. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 19, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"/blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications-amp-conferences",title:"/publications &amp; conferences",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-community-engagement",title:"/community engagement",description:"Being able to play my part in making the community more accessible for everyone, more fun is important to me. Some things I have had the pleasure of being a part of.",section:"Navigation",handler:()=>{window.location.href="/service/"}},{id:"nav-cv",title:"/cv",description:"Please find attached my resume (linked) and CV.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-reshaping-bonsai",title:"Reshaping Bonsai",description:"Pruning LLMs for Mathematical Reasoning. Can we prune LLMs while maintaining their mathematical reasoning abilities? How does a novel comprehensive metric affect pruning?",section:"Posts",handler:()=>{window.location.href="/deep-learning/nlp/llm/pruning/2024/05/21/reshaping-bonsai.html"}},{id:"post-visual-prompt-tuning",title:"Visual Prompt Tuning",description:"Can you transfer prompts? What is the best place to append prompts? Do they increase the adversarial robustness? Find out here :)",section:"Posts",handler:()=>{window.location.href="/deep-learning/computer-vision/visual-prompt-tuning/2024/05/20/visual-prompt-tuning.html"}},{id:"news-started-working-in-prof-emma-s-lab",title:"Started working in Prof. Emma\u2019s Lab",description:"",section:"News"},{id:"news-taing-advanced-nlp-11-711-with-prof-graham-neubig",title:"TAing Advanced NLP (11-711) with Prof. Graham Neubig",description:"",section:"News"},{id:"news-our-work-on-tacking-throughput-latency-tradeoff-with-speculative-decoding-accepted-to-eecv-24",title:"Our work on tacking throughput-latency tradeoff with speculative decoding accepted to EECV \u201824...",description:"",section:"News"},{id:"projects-visual-prompt-tuning",title:"Visual Prompt Tuning",description:"Can you transfer prompts? What is the best place to append prompts? Do they increase the adversarial robustness? Find out here :)",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-reshaping-bonsai",title:"Reshaping Bonsai",description:"Pruning LLMs for Mathematical Reasoning. Can we prune LLMs while maintaining their mathematical reasoning abilities? How does a novel comprehensive metric affect pruning?",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%76%61%73%68%69%73%74%74@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=vgt6Y5wAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/vashistht","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/vashistht","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>