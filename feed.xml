<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://vashistht.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vashistht.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-08T21:07:58+00:00</updated><id>https://vashistht.github.io/feed.xml</id><title type="html">blank</title><subtitle>this is the personal website of Vashisth Tiwari / Vash Tiwari, a graduate student at Carnegie Mellon and a graduate of University of Rochester.</subtitle><entry><title type="html">[Blog with Together.ai] Speculative decoding for high-throughput long-context inference</title><link href="https://vashistht.github.io/2024/09/05/blog-with-togetherai-speculative-decoding-for-high-throughput-long-context-inference.html" rel="alternate" type="text/html" title="[Blog with Together.ai] Speculative decoding for high-throughput long-context inference"/><published>2024-09-05T00:00:00+00:00</published><updated>2024-09-05T00:00:00+00:00</updated><id>https://vashistht.github.io/2024/09/05/blog-with-togetherai-speculative-decoding-for-high-throughput-long-context-inference</id><content type="html" xml:base="https://vashistht.github.io/2024/09/05/blog-with-togetherai-speculative-decoding-for-high-throughput-long-context-inference.html"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">[Blog with Infini AI Lab] MagicDec: Breaking the Latency-Throughput Tradeoff for Long Contexts with Speculative Decoding</title><link href="https://vashistht.github.io/2024/08/23/blog-with-infini-ai-lab-magicdec-breaking-the-latency-throughput-tradeoff-for-long-contexts-with-speculative-decoding.html" rel="alternate" type="text/html" title="[Blog with Infini AI Lab] MagicDec: Breaking the Latency-Throughput Tradeoff for Long Contexts with Speculative Decoding"/><published>2024-08-23T00:00:00+00:00</published><updated>2024-08-23T00:00:00+00:00</updated><id>https://vashistht.github.io/2024/08/23/blog-with-infini-ai-lab-magicdec-breaking-the-latency-throughput-tradeoff-for-long-contexts-with-speculative-decoding</id><content type="html" xml:base="https://vashistht.github.io/2024/08/23/blog-with-infini-ai-lab-magicdec-breaking-the-latency-throughput-tradeoff-for-long-contexts-with-speculative-decoding.html"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[MagicDec: Breaking the Latency-Throughput Tradeoff for Long Contexts with Speculative Decoding]]></summary></entry><entry><title type="html">Reshaping Bonsai</title><link href="https://vashistht.github.io/deep-learning/nlp/llm/pruning/2024/05/21/reshaping-bonsai.html" rel="alternate" type="text/html" title="Reshaping Bonsai"/><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://vashistht.github.io/deep-learning/nlp/llm/pruning/2024/05/21/reshaping-bonsai</id><content type="html" xml:base="https://vashistht.github.io/deep-learning/nlp/llm/pruning/2024/05/21/reshaping-bonsai.html"><![CDATA[<p>You can find more about the project in the paper here <a href="/assets/pdf/ReshapingBonsai.pdf" target="_blank">Click here to view the full PDF</a>.</p> <h2 id="introduction">Introduction</h2> <p>This project, completed for the Advanced Natural Language Processing course, focused on improving the Bonsai <d-cite key="dery2024everybodyprunenowstructured"></d-cite> pruning method for Large Language Models (LLMs) with a specific emphasis on mathematical reasoning capabilities.</p> <h3 id="key-concepts">Key Concepts</h3> <ol> <li> <p><strong>Bonsai Pruning</strong>: A forward-only, regression-based neural network pruning method that decides which modules to prune based on estimates of module importance.</p> </li> <li> <p><strong>Comprehensive Metric</strong>: A novel metric combining lexicographical similarity, semantic similarity, and accuracy to evaluate model-generated outputs against ground truth during pruning.</p> </li> </ol> <h2 id="technical-background">Technical Background</h2> <h3 id="bonsai-pruning-method">Bonsai Pruning Method</h3> <p>The Bonsai pruning method aims to solve the following optimization problem:</p> \[m^* = \arg\max_{\bar{m} \in F_p} U(M|_{\bar{m}})\] <p>where \(F_p = \{\bar{m} \subseteq m \mid \sum_{[j:m_j \in \bar{m}]} s_j \leq (1-p)D\}\)</p> <p>Here, \(m^*\) represents the optimal sub-model, \(p\) is the target sparsity, \(U\) is the utility function measuring model performance, and \(D\) is the total number of parameters.</p> <p>Bonsai estimates module importance using a regression-based approach:</p> \[\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^N} \left\{\frac{1}{n}\sum_{(\bar{m}_k, U_k) \in \mathcal{D}} (U_k - \beta^T \alpha_{\bar{m}_k})^2 + \gamma\|\beta\|^2\right\}\] <p>where \(\mathcal{D}\) is the dataset of sampled sub-modules and their performances, and \(\alpha_{\bar{m}_k}\) is a binary mask.</p> <p><strong>Limitation:</strong> Bonsai shows great promise based on its performance on 4/6 tasks on Huggingface Open LLM Leaderboard in its parameter category. However, one notable exception to this generally good performance is its performance on the GSM-8K dataset, which is a mathematical reasoning dataset (achieving ~6% accuracy in its best hyperparameter setting). In this work, we wanted to see if we can improve its performance on mathematical reasoning tasks.</p> <h3 id="our-novel-comprehensive-metric">Our Novel Comprehensive Metric</h3> <p><strong>Key Insight:</strong> Our key insight was to notice that while the usual gradient pruning requires the metric \(U\) to be differentiable, this regression-based approach allows us to use any <em>well-defined</em> metric, so long as we can obtain a good estimation of the module’s importance. For instance, while accuracy is not differentiable, it can still be used here.</p> <blockquote> <p>We asked whether a metric that rewards better reasoning during pruning help with the downstream performance. How can we come up with this metric?</p> </blockquote> <p>Building on this insight, we experimented with combining accuracy (to capture the quality of the final output), lexicographical similarity (to ensure intermediate numbers are correct), and semantic similarities (to capture the similarity in meaning) between the true and generated tokens.</p> <p>We introduce a new metric \(U^\dagger\) that combines lexicographical similarity, semantic similarity, and accuracy: \(U^\dagger = \sum_{i=1}^n a_i M_i \quad \text{where} \quad \sum_{i=1}^n a_i = 100\)</p> <p>Here, \(M_i\) represents individual metrics (e.g., lexicographical similarity, semantic similarity, accuracy), and \(a_i\) are their respective weights.</p> <h2 id="research-focus">Research Focus</h2> <p>The project explored various aspects of LLM pruning through several experiments and ablation studies:</p> <ul> <li> <p><strong>Task-Specific Pruning:</strong> Used the GSM-8K dataset, which includes step-by-step reasoning for math problems, to guide the pruning process.</p> </li> <li> <p><strong>Chain-of-Thought Prompting:</strong> Incorporated example questions and “think step by step” instructions in our prompts during pruning. Our prompt structure was:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Example:{example-question}, Rationale: {example-rationale}, Answer: {example-answer}.
Question: {new-question}
"Let's think step by step to get the rationale and the answer:"
</code></pre></div> </div> </li> <li> <p><strong>Sparsity Levels:</strong> Investigated model performance at different sparsity levels, particularly at 50% and 60% retention.</p> </li> <li> <p><strong>Metric Component Analysis:</strong> Examined the impact of different components in our comprehensive metric:</p> <ul> <li>Lexicographical Similarity (lex_sim): Calculated using F1 score</li> <li>Semantic Similarity (cos_sim): Computed using cosine similarity of sentence embeddings</li> <li>Accuracy (acc): Measured using exact match</li> </ul> </li> </ul> <h2 id="key-findings">Key Findings</h2> <ul> <li> <p>Pruning with GSM-8K led to improved performance on 5 out of 8 downstream reasoning tasks, particularly for logical reasoning datasets.</p> </li> <li> <p>At 60% retention (40% sparsity), our pruned model outperformed the baseline across most datasets, with notable gains in Arc-e, HellaSwag, and MMLU-Elementary Math.</p> </li> <li> <p>For specific weight combinations (38-24-38 for lex_sim, cos_sim, and acc respectively) in our metric, we achieved a 20x lower perplexity on GSM-8K compared to the baseline:</p> <table> <thead> <tr> <th>Model</th> <th>GSM-8K Perplexity</th> </tr> </thead> <tbody> <tr> <td>Baseline</td> <td>365.5226</td> </tr> <tr> <td>Our Model (38-24-38)</td> <td>17.1083</td> </tr> </tbody> </table> </li> <li> <p>Exact match scores (used as a proxy for accuracy) were often close to 0 at high sparsity levels, while semantic similarity scores were consistently high, suggesting potential limitations in our embedding model.</p> </li> </ul> <h2 id="challenges-and-limitations">Challenges and Limitations</h2> <ol> <li> <p><strong>Computational Constraints:</strong> We had to reduce the number of generated tokens from 100 to 20 and increase pruning step size from 5% to 20% per iteration.</p> </li> <li> <p><strong>Accuracy Measurement:</strong> Defining accuracy based on the presence of the ground truth answer string in the output may have led to false positives.</p> </li> <li> <p><strong>Embedding Model Limitations:</strong> The sentence embedding model used for semantic similarity (all-MiniLM-L6-v2) may not have captured the nuances of mathematical reasoning effectively.</p> </li> </ol> <h2 id="future-directions">Future Directions</h2> <ul> <li>Implement more granular pruning steps</li> <li>Explore richer embedding models for semantic similarity, possibly using LLaMA’s own embeddings</li> <li>Conduct thorough exploration of hyperparameters</li> <li>Investigate Bayesian Linear Regression for choosing regression coefficients</li> <li>Explore non-linear combinations of metric components</li> </ul> <p>While our results were mixed, this project demonstrates the potential of using task-specific datasets and comprehensive metrics for pruning language models while maintaining their reasoning capabilities. As we continue to refine our approach, we hope to contribute to the development of more efficient and capable language models for specific reasoning tasks.</p>]]></content><author><name>Vashisth Tiwari</name></author><category term="deep-learning"/><category term="nlp"/><category term="llm"/><category term="pruning"/><category term="projects"/><summary type="html"><![CDATA[Pruning LLMs for Mathematical Reasoning. Can we prune LLMs while maintaining their mathematical reasoning abilities? How does a novel comprehensive metric affect pruning?]]></summary></entry><entry><title type="html">Visual Prompt Tuning</title><link href="https://vashistht.github.io/deep-learning/computer-vision/visual-prompt-tuning/2024/05/20/visual-prompt-tuning.html" rel="alternate" type="text/html" title="Visual Prompt Tuning"/><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://vashistht.github.io/deep-learning/computer-vision/visual-prompt-tuning/2024/05/20/visual-prompt-tuning</id><content type="html" xml:base="https://vashistht.github.io/deep-learning/computer-vision/visual-prompt-tuning/2024/05/20/visual-prompt-tuning.html"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This project, completed for the Introduction to Deep Learning course, focused on Visual Prompt Tuning (VPT) in Vision Transformers (ViT) <d-cite key="jia2022visualprompttuning"></d-cite>. We explored the effectiveness and applications of VPT through various experiments and ablation studies. You can find a short summary of our project in this <a href="https://youtu.be/dUh8MXHKvNE?si=efeuhlRyoUFlVIlB">video presentation</a>.</p> <h2 id="key-concepts">Key Concepts</h2> <p><strong>Vision Transformer (ViT)</strong>: A neural network architecture that applies the transformer model, originally designed for natural language processing, to image analysis tasks.</p> <p><strong>Visual Prompt Tuning (VPT)</strong>: A technique that uses continuous vectors in the embedding or pixel space. It involves a frozen transformer backbone and task-specific prompts that are updated during training.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/visual-prompt-tuning-480.webp 480w,/assets/img/visual-prompt-tuning-800.webp 800w,/assets/img/visual-prompt-tuning-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/visual-prompt-tuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="vpt-image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <stong>Figure 1:</stong> A schematic of VPT where we add learnable parameters in the embedding or pixel space. While training these, the transformer backbone kept frozen and only the task specific prompts are updated <d-cite key="jia2022visualprompttuning"></d-cite> </div> <h2 id="research-focus">Research Focus</h2> <p>Our project explored various aspects of VPT through several experiments and ablation studies:</p> <ol> <li>Prompt Placement</li> <li>Prompt Size</li> <li>VPT-Deep Layer Depth</li> <li>Adversarial Robustness</li> <li>Transfer Learning</li> </ol> <h2 id="experiments-and-results">Experiments and Results</h2> <h3 id="prompt-placement">Prompt Placement</h3> <p>We compared three different approaches for prompt placement:</p> <ul> <li>Prepending prompts at the pixel layer</li> <li>Prepending prompts at the embedding layer</li> <li>Adding prompts element-wise to the embedding layer</li> </ul> <p>Results showed that prepending to the embedding layer yielded the best performance.</p> <h3 id="prompt-size">Prompt Size</h3> <p>We conducted a sweep from 25 to 150 tokens to determine the optimal prompt size. Key findings include:</p> <ul> <li>Adding prompts significantly increased accuracy</li> <li>50 and 125 tokens showed similar and highest accuracy</li> </ul> <h3 id="vpt-deep-layer-depth">VPT-Deep Layer Depth</h3> <p>We investigated the impact of the number of transformer encoder layers to which learnable prompt parameters were prepended.</p> <h3 id="adversarial-robustness">Adversarial Robustness</h3> <p>We tested the model’s resilience to input noise, demonstrating that adding a prompt universally increased model robustness to noisy inputs.</p> <h3 id="transfer-learning">Transfer Learning</h3> <p>We explored if prompts trained on one dataset (CUB-200) could provide better initialization than standard methods when applied to a new dataset (Stanford Dogs).</p> <h2 id="key-findings">Key Findings</h2> <ol> <li>Prepending prompts to the embedding layer yielded the best performance, possibly allowing the prompt to learn condensed characteristics.</li> <li>The addition of prompts led to a significant increase in accuracy, with minimal differences across various prompt sizes.</li> <li>VPT demonstrated increased robustness to noisy inputs compared to models without prompts.</li> <li>Transfer learning experiments showed promise in using pretrained prompts as initialization for new tasks.</li> </ol>]]></content><author><name>Vashisth Tiwari</name></author><category term="deep-learning"/><category term="computer-vision"/><category term="visual-prompt-tuning"/><category term="projects"/><summary type="html"><![CDATA[Can you transfer prompts? What is the best place to append prompts? Do they increase the adversarial robustness? Find out here :)]]></summary></entry></feed>